# Test-Driven Development in JavaHere are some notes to help you get the most from the TDD course.## 1. Github instructionsWe'll be using Github for sample code and starter code for programming practice. You'll need a Github ID so you can commit to the repository for the course.If you don't already have one, sign up for a Github account here:https://docs.github.com/en/get-started/signing-up-for-github/signing-up-for-a-new-github-accountThe instructions direct you to a Pricing page. Choose Free (unless you prefer one of the paid options for your own reasons).Then you have to verify your email address. Instructions are here: https://docs.github.com/en/get-started/signing-up-for-github/verifying-your-email-address Once that's done, let the instructor know your Github ID so they can invite you to the Github repository containing starter code for the class. Finally, you'll need to accept the invitation to collaborate in the repository. ## 2. Notes on the topics in the course outline### 2.1 What is TDD? TDD stands for Test-Driven Development or Test-Driven Design, depending on whom you ask. It's an approach to building code that supports emergent design and incremental development. TDD is most useful when writing code "from scratch" or "by hand" as opposed to using a code generator or wiring up functions provided by a framework or a runtime environment. Please note that I haven't described TDD as a testing technique. The test suite that results from it does play a role in testing in that it is the first test suite executed in the continuous integration process, but TDD is fundamentally a software design/development technique rather than a testing technique. This is a common misconception.#### 2.1.1. Benefits* Why adopt TDD? * Benefits of TDD* Impact of adopting TDDThese topics are all synonymous. The following video may help answer these questions. It introduces various benefits of TDD in the context of using TDD to develop a small application. (Apologies in advance for the audio quality.) https://www.youtube.com/watch?v=Bf89rd0o5-0 There are additional benefits beyond those mentioned in the video. **Reducing the cost and risk of change**. Any change to software introduces risk - the risk of creating a new bug. Changes also incur cost. Development techniques that minimize risk and cost are often desirable. TDD reduces the risk and cost of making changes to software, mainly because it includes incremental refactoring. That prevents "cruft" from building up in the code base, keeping the cost of change relatively stable over the lifetime of the product, and reducing the chances that a modification will introduce a regression. **Reducing the chances of over-engineering or gold-plating the solution.** Following the TDD cycle (defined below) rigorously, we end up with "just enough code" to solve the problem and we keep the design as simple as is practical, through incremental refactoring (defined below). Using traditional methods, there's a good chance our solution will contain more code than necessary and the code will be relatively difficult to follow and understand.Besides all that, teams that use TDD as their default way of working on code enjoy time savings in the following ways. **Time-saver #1**. The unit-level test suite functions as low-level documentation of the code base. Since the test suite and the production code are always committed to version control together, and if we assume the team is self-disciplined about always writing test cases before touching the production code, this form of "documentation" can never get out of sync with the code. This is much more effective than separate documentation and/or source comments. It saves time because the executable documentation is created as a natural part of the development process and requires no additional effort. **Time-saver #2**. The unit tests also function as a low-level regression test suite. After changing the code, should any test cases fail it indicates we have introduced a regression. There is no need for a separate effort to create a regression test suite (at the unit level, anyway), or for the team to spend time performing regression testing as a separate activity from the normal development workflow. In organizations that have separate testing teams, this aspect of TDD also eliminates the time for coordinating the work between developers and testers (for unit-level verification, not for all forms of testing).**Time-saver #3**. When a product stakeholder, team member, or anyone else in the organization needs to know the current state of the code or they need to learn what the application does, they can run the executable test suites (unit-level and others). They don't have to interrupt the team to ask about this or to request a demo. It's self-service.**Time-saver #4**. When code is built using TDD, the resulting design tends to align with generally-accepted good software design principles without any special effort. This is an effect of writing code that can easily be isolated for microtesting. It happens naturally and eliminates most of the time we might otherwise spend later on to improve the solution architecture and design, or to recover from unexpected "gotchas" arising from overlooked details during up-front design.**Time-saver #5**. When new team members are onboarded, they can come up to speed on the code base much faster by reading and executing test suites than is possible by reading documentation or production source code. Otherwise, team members must take some time away from value-add work to show new team members the ropes. The time-saving effect is multiplied when the team routinely uses collaborative methods in their normal workflow, such as ensemble programming or pair programming. Training sessions are less effective for this purpose because things don't usually work the way they are presented in the sessions, and also because it's hard for people to remember a lot of details presented in the abstract. New team members don't have an opportunity to see how things actually work until they start putting their hands on the code. When they learn through executable test suites while collaborating with team mates on real work, they put their hands on the code immediately, and without interrupting product development flow.#### 2.1.2. VocabularyThere are some jargon terms related to TDD you will often read or hear. The following terms are not presented in alphabetical order. They are presented in an order that lets us build our understanding piece by piece.**SUT, system under test, CUT, code under test**. The subset of the application that we intend to exercise in a test case.**Testing vs. checking**. For historical reasons, we call every kind of verification of software functionality "testing," but that term is broad. It's often helpful to remember that testing, in the true sense, is an exploration of what we don't already know about the SUT. We are trying to discover information about the SUT, such as sequences of user actions that result in unexpected output, or operational limits of the SUT like memory constraints or performance issues, or counterintuitive UI behavior that may confuse users, or accessibility or localization problems, or intermittent issues that occur due to misconfiguration, race conditions, or interaction between software components that aren't apparent from automated checks. Testing may be aided by software tooling, including machine learning, but it is always a human-driven, thoughtful, creative process. Each step is guided by the results of previous steps. We discover new questions as we progress. We can't really automate testing in the true sense of the word, because we are operating in an unknown or poorly-understood space.When we write unit tests for execution, whether with TDD or after the fact, we are verifying, or checking, that the SUT behaves the way we expect it to behave under controlled conditions. Many professional software testers are careful to use the word _checking_ when they mean this kind of activity, and the word _testing_ when they mean exploring unknown aspects of the system; but most people in the software industry use the word _testing_ for everything. See https://www.satisfice.com/blog/archives/856 for more information.Sometimes unknowns reveal themselves through failed test cases, but the intention is to check known (or at least, expected) behaviors of the SUT rather than discover unknown ones. People use the word "testing" for both types of work, but we should remember the difference. **Example-based tests**. Each test case is a concrete example of a behavior we expect the SUT to exhibit when it is in a given state and it receives given input values. Some people call these test cases _examples_. We don't try to check multiple behaviors of the SUT in a single example. If the code contains conditional logic, then we write a separate example for each path through the conditional logic. If the code can throw exceptions, then each exception is checked with a separate example. **Test-Driven Development, Test-Driven Design (TDD)**. As TDD is based on concrete examples of expected SUT behavior, it is never "testing" in the true sense of the term. Unfortunately, the word "test" was baked in from Day One. During the development process, when new test cases drive modifications to the production code, TDD operates as a software design/development technique. After the code exists, the resulting unit test suite operates as a regression test suite (there's that word again) that checks whether we have introduced any regressions.**Parameterized tests or data-driven tests**. These are also examples, but we feed them a list of inputs and expected outputs so that we don't have to write and maintain a long list of nearly-identical examples separately. We'll see examples of this in the course, and you may well write some yourself. See https://www.baeldung.com/parameterized-tests-junit-5 for more information.**Property-based tests**. With property-based testing, or PBT, we use special tooling that generates input values automatically. PBT is not always applicable to TDD, but it is sometimes possible to define properties of the SUT in advance and use PBT to drive the code. In other cases, PBT can be useful after code has been developed to ferret out edge cases that we didn't think of when we used TDD to write example-based test cases. We'll show a simple example of PBT, but it's out of scope for the hands-on part of the course. See https://hypothesis.works/articles/what-is-property-based-testing/ for more information. **Mutation tests**. Given a clean test suite - that is, all the tests are passing - we can use special tooling to perform mutation testing. Mutation testing is not used directly for TDD, but many teams that practice TDD also use mutation tests to look for "holes" in their test suite. Mutation testing tools modify the SUT to create bugs - or at least, behaviors that are not covered in the test suite - so that we can see whether we need to enhance our test suite. Each modification is called a _mutant_, and the goal is that our test suite kills all mutants (that is, detects all the changed behaviors). If any mutants survive, it means our test suite is missing one or more examples to cover those situations. See https://en.wikipedia.org/wiki/Mutation_testing for more information.**Approval tests**. When we first work with a legacy code base that lacks executable test suites, it's often helpful to record the current behavior of the code as best we can so that we can detect changes in the behavior as we begin to refactor the code and/or add features to the product. We execute the SUT to produce whatever output it creates - typically a file or data loaded/modified in a database - and save that result. The result is called the "golden master". We then use the golden master as a reference to see whether any code changes we made affected the behavior of the SUT. This is not part of TDD as such, but teams often use this technique to get started with an existing code base. See https://approvaltests.com/ for more information.**Unit test**. This is a test case or example that exercises a single unit of code. Unfortunately, there are two problems with this term. First, there's no standard definition of a "unit" of code. It can be any amount of code. If our unit is large and contains a lot of different functionality, then it can be time-consuming to track down the cause of a test failure. So, a rule of thumb is that a "unit" of code is small. But how small is small enough? The answer can depend on the characteristics of the programming language in use and on how easy or hard it is to isolate small functions within the SUT. We should cultivate the habit of going as small as we can. See https://www.leadingagile.com/2018/11/test-scope-whats-the-scope-of-a-unit-test/ for more information.Second, many testing libraries and frameworks include the word "unit" in their names - like JUnit, NUnit, XUnit, pyUnit, zUnit, and so on. That's fine, except that many people get into the habit of referring to any test case that happens to be written using one of these libraries as a "unit test" just because the tool has the word "unit" in its name. That isn't what makes a unit a unit. It's just a name. We can write integration tests and system tests and do other things with "unit" testing tools, like probing existing code to find the source of a bug, or learning what an unfamiliar piece of code returns when we call it with various input values. It's the scope and purpose of an example that makes it a unit test or some other kind of test, not the name of the library we used.**Microtest**. The term microtest was coined to avoid the ambiguity of the term unit test. The name implies we intend to check the smallest subset of code we can. For Java, this is typically a single logical path through a single method. For other languages, it could be a smaller or larger chunk of code, depending on how the language works. See https://www.geepawhill.org/2018/04/16/the-technical-meaning-of-microtest/ for more information.**Code isolation**. We want to minimize the amount of time and effort required to understand the cause of each test failure. The goal is to design our examples such that each example can fail for exactly one reason - the actual behavior does not match the expected behavior. We don't want to write examples that can fail due to a network timeout or missing test data or invalid user credentials or an incorrect configuration setting or a bug in a library function the SUT calls. We want to check a very small subset of functionality, and that's all. That way, when an example fails, we can go straight to the line of code that caused the problem without spending much time in analysis.**Test doubles**. When code is created from the outset using emergent design guided by TDD, each testable unit of code can be naturally very small and isolated from dependencies. But most of the code we work with was not created using TDD, and/or has been modified over the years by people who did not use TDD to make the changes. In those situations, the smallest practical unit we can test may yet have external dependencies that have nothing to do with the behavior we intend to check. See http://xunitpatterns.com/Test%20Double.html for more information.We use test doubles to fence off the dependencies that are external to the unit of interest in a particular example. Some people have coined lots of different names for test doubles that serve different purposes, but as a practical matter there are a couple of different kinds: _mocks_ and _spies_. The test double stands in for a dependency in the same sense as a stunt double stands in for an actor in an action movie. When the SUT calls the dependency it actually talks to the test double instead, and doesn't know it. A _mock_ is a test double that always returns a predefined value. In addition, most mocking libraries also support counting the number of invocations of the mock by the SUT. Mocks are used to make the SUT follow the logical path through the code that we need it to follow for purposes of a given example, by ensuring the SUT always gets the response from each dependency that goes with the situation we're checking. Because Java is based on classes, we have to create a mock for a whole class instead of just a single method, as we might prefer to do. When the application is based on Java interfaces, it's easy enough. The mock will be an "empty" implementation of the interface; it doesn't contain any of the logic of a "real" instance. The mocked method will return whatever we tell it to return, when called. But in some cases we have to fence off part of a dependency; the classes are a little bloated (in particular, they violate the Single Responsibility Principle or SRP), and the SUT needs some of the functionality of the real instance while mocking other functionality. This is not ideal, but most mocking libraries provide another type of test double, called a _spy_, to handle it. A spy instantiates the class and substitutes a mocked method for the method we intend to fake. Other methods on the spy are real. **Assertion**. An assertion is a statement of the expected behavior of a microtest or unit test (or any other check). It's along the lines of, "When we add 2 and 5 we get 7." If the SUT adds 2 and 5 and returns something besides 7, the assertion fails.**Test automation, CI, CD, pipeline**. The terms _test automation_ and _automated tests_ are used loosely in the industry to mean any test cases that are executable, but a more precise meaning is that the test is executed automatically (without human intervention) each time a triggering event occurs. Normally in our line of work, we commit changes to a source code repository, and the commit becomes the triggering event for a series of actions that we may call a _continuous integration pipeline_ or a _continuous delivery pipeline_. As long as all the tests pass, the predefined steps in the pipeline are executed automatically. As soon as something fails, the pipeline stops and some form of notification is issued so the team can correct the problem. There's no point in going any further until we determine the cause of the problem and fix it. For a test case to be automated, it has to be able to run _unattended_. That means there's no need for a human to watch the test run in order to respond to prompts, like logins and URIs and so forth. For a test case to run unattended, it has to be _executable_. Rather than a documented test plan or list of steps for a person to follow manually, the actions required to perform the test must be scripted or programmed in some way.For a test case to be executable, is has to be _repeatable_. Every time the test is performed, it is identical. This attribute of executable examples is what enables the test suite to signal us when we introduce a regression. For a test case to be repeatable, it has to be set up so that it does not require any set-up or modification in between executions, or any test data reload or reset. We can run it again and again as is. For a test case to be independent of set-up steps, the SUT has to be _isolated_ from external dependencies whose state is not guaranteed at the time we need to do the test. At the _unit test_ or _microtest_ level, this means the SUT is isolated from _every_ external dependency, including the file system; microtests do not read test data from an external source.So, depending on how things are set up currently in a given team or organization, there may be a series of things that have to be done to get the test suites into proper shape for test automation.**Test automation pyramid**. This is a popular triangular diagram that shows microtests or unit tests at the bottom, as a foundation, with tests of progressively larger scope layered on top of that. The implication of the triangular shape is that we will have a large number of small examples at the bottom (often called unit tests or microtests), a smaller number of slightly larger examples above that (often called functional tests or integration tests or some other name; there are no standard names), and so on until we reach the top of the pyramid, where we are exercising the entire system with all its external interfaces live. At each successive layer, we trust the layer below to cover everything that can reasonably be covered at that level, so we don't repeat the same assertions again at higher levels. This is useful, because tests of large scope that involve databases and files and network services and what-not are more time-consuming to set up and execute than tests of small scope that exercise small, well-isolated units of code with no external data feeds or live connections. Also, when a test case of large scope fails, it can be time-consuming to track down the reason. See https://martinfowler.com/articles/practical-test-pyramid.html for more information. **Programmer test**. This term refers to executable test cases that are commonly written by programmers rather than software testing specialists. Programmers are usually responsible for microtests or unit tests because those must be written in the same programming language as the SUT, so that the examples can access the fine-grained results they need to check, such as values returned from individual methods or functions. A second reason is that the SUT is fully isolated, so there will be no files or databases we can look at to verify the outcome of test cases.Besides the scope of the tests, microtests are used with TDD to drive the development of the production code; that is not a responsibility of testing specialists. In teams that have multi-skilled members, there's no distinction between programmers and testers, so the term "programmer test" might seem redundant or unnecessary; but many teams today still have those separate roles.There is overlap between the responsibilities of programmers and testers for writing executable tests at the level of functional and integration tests. This sometimes causes confusion. It may be helpful to remember that TDD as such really pertains only to the unit or microtest level. A more general concept along the lines of "begin with the end in mind" may apply at higher levels, and may involve test automation tooling, but strictly speaking TDD is for low-level code design/development. For example, a traditional test plan that is based on a requirements document aligns, on a conceptual level, with the idea to "begin with the end in mind," but it is in no way, shape, or form TDD.**Red-Green-Refactor; also known as the TDD Cycle**. When applying TDD, we begin by writing a single example that expresses a single concrete behavior that we would like the SUT to exhibit. We run the example to make sure it fails for the right reason. That is, it doesn't fail because we forgot to set something up in the test case, but rather because the SUT doesn't exhibit the behavior the example asserts. Most unit testing tools display output in red for failed examples, and in green for passing examples. That's why we call this the "red" step in the cycle. With the example failing for the right reason, we write the minimum amount of code to cause the example to pass, or turn "green." We only write enough code to satisfy the one example without breaking any of the existing examples. Even when we know this isn't sufficient to satisfy the requirements, the process is to build up the functionality one example at a time, and not rush ahead.Once we're "at green," we examine the production code and the test code to look for opportunities to simplify the design. If we decide to refactor, then we run the test suite again afterwards. If we inadvertently changed the behavior of the SUT while refactoring, one or more test cases will fail. We clean that up if necessary until we're at green again. Then we repeat the cycle.**Refactoring**. This means changing the internal design of code without changing its observable behavior. During the TDD cycle we refactor incrementally, either to "clean up" after making a change or to help set things up for the next change we intend to make. It is also common to refactor independently of the TDD cycle to improve the design at a larger scale, should we determine that is necessary. In that situation, we would want to check the behavior of the SUT before and after refactoring using tests at a higher level of abstraction than the unit level. See https://www.refactoring.com/ for more information.**Behavior-Driven Development, BDD, behavior-oriented tests**. The term BDD was coined to emphasize the value of focusing on the observable behavior of the SUT rather than on its technical implementation. There are BDD tools that express preconditions, actions, and postconditions using terms that naturally suggest the focus on behavior vs. implementation. For general unit testing tools, we still want to take a behavior-oriented approach to test case design so that we will not have test failures due to changes in the underlying implementation. Cases that fail due to changes in implementation details aren't useful for detecting regressions, because they will fail even when the behavior of the SUT does not change.We won't have time to get into this during the course, but there are different styles of assertions and we can choose assertion libraries that support the style(s) we prefer. Some of these are more obviously behavior-focused than others. Regardless of the assertion style we're using, our examples will be more useful if they don't have dependencies on the underlying implementation details of the SUT. See https://dannorth.net/introducing-bdd/ for more information.### 2.3 How to adopt TDD Subtopics on the outline include:* Infrastructure Changes Required * Adoption Planning* Measuring and Monitoring Adoption * Adjusting This list might look a bit intimidating, but actually it isn't too hard to begin using TDD. You could actually start using it the next time you touch your code base. That might seem counterintuitive, since you're probably visualizing the code you've inherited, and it doesn't look like it would be easy to isolate small chunks of functionality for unit testing. That's not a show-stopper, because TDD is applied in conjunction other contemporary development practices, like emergent design, iterative development,  and refactoring. You can make small changes, one step at a time, to accommodate TDD safely. It isn't necessary to achieve full code coverage immediately. As long as you're moving in the right direction, every improvement is good.#### 2.3.1 Infrastructure Changes RequiredThere are a couple of considerations here. First, there's setting up your development environment with the tooling necessary to support TDD. Second, there's configuring your CI/CD pipeline tooling to collect the test results and ship them to some target environment where people can review them, and possibly to notify the development team of any build failures - although you may already have notification in place to support your existing pipeline.TDD fits into a CI/CD pipeline right at the beginning. Your development environments and the script your CI server executes when you commit changes are the only components of the pipeline the unit test suite touches. Those are usually under the direct control of development teams, or at worst, you have an infrastructure support team who can help you get set up.The two market-leading Java IDEs (JetBrains IntelliJ IDEA and Eclipse) ship with support for JUnit already bundled. If you prefer a different test library, there are others you can define as dependencies in Gradle, Maven, or the dependency manager of your choice. JUnit has an assertion library bundled. If you prefer a different one, there are others you can define as dependencies. The IDEs don't ship with mocking libraries, mutation test libraries, or PBT libraries bundled, but all these things can be added to your set-up by defining dependencies in your build. For large or complicated applications, you may configure your build to run tests in parallel on multiple servers. In that case, you will need to set up the pipeline infrastructure to aggregate the results of the test runs and to stop the pipeline if any one of them reports a test failure. For simpler cases, there really isn't much, if any infrastructure work to do downstream of the development environment.#### 2.3.2. Adoption PlanningTDD is a basic software development technique. It's almost on the level of learning how to write "if" statements. You don't really need much planning before you can start writing "if" statements. You just do it. #### 2.3.3. Measuring and Monitoring AdoptionTeams that are new to TDD (or to any other particular technique) often find it helpful to track the use of the technique so they are sure people are picking it up and using it. Simple metrics like code coverage can be helpful. Look for an upward trend in the percentage of code that's covered. But be careful not to fall into the trap of writing worthless tests just to trick the metrics. That isn't the point. Once TDD has become part of the normal workflow, you can stop tracking coverage. Team collaboration is the most effective way to support adoption of unfamiliar development techniques. Through ensemble programming, pair programming, or code reviews (depending on how the team operates), team members can remind each other to follow the TDD cycle, to work in small increments, and to refactor as they go along.Measuring _adoption_ of TDD is one thing, and measuring the _effects_ of TDD and other development practices is another. Code coverage metrics as such don't tell us much about the health of the code base. There may be test cases that pass, but don't tell us anything useful about the application. In fact, some people write test cases just to influence the metrics. They are required to hit some arbitrary percentage of code coverage, so they make darn sure they do so. These cases bloat the test suite without adding value. There may be test cases that are set to be ignored because they tend to cause problems. Often, these depend on external dependencies that really shouldn't be part of unit testing in the first place. When the dependency is in the wrong state for the test case, the case fails. That failure is misleading because it doesn't indicate anything about the behavior of the SUT one way or the other. The value of code coverage metrics depends heavily on the quality of the test suite. Setting a target for percentage of tests that pass is not useful. All the tests should pass. Otherwise, the test suite is useless. If you have meaningless tests, remove them. If you have poorly-designed tests that are unreliable, fix them. It's better to have one test case that actually means something than to have 10,000 tests that just make the metrics look good.Avoid setting a target for code coverage. If you track code coverage, consider plotting it over time to see if the trend is positive. Moving in the right direction over time is more important than hitting arbitrary targets. Even then, there are more useful metrics than code coverage for that purpose. That's out of scope for this course, but have a look at tools like SonarQube for more ideas.#### 2.3.4. Adjusting As you and your team work more with TDD and related practices, you will probably learn how to do these things more and more effectively. It's a Good Thing to think about and discuss how the team might adjust its use of technical practices to achieve better results. There's no law that says you have to understand the "best" way to use TDD up front, and then never change. ### 2.4. The rest of the outline The remainder of the outline mentions topics already covered in this document, and that we'll deal with through hands-on work during the course. 